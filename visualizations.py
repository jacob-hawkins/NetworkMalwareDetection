import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.tree import plot_tree
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from mpl_toolkits.mplot3d import Axes3D
import os

def createCorrMatrix(df, axislabel, title, filename):
    os.makedirs('./results', exist_ok=True) 

    plt.figure(figsize=(10,7))
    ax = sns.heatmap(df, annot=True, yticklabels=axislabel, xticklabels=axislabel, cmap='Blues')
    ax.set(title=title)

    plt.savefig(f'results/{filename}.png')

def createPieChart(percentages, labels, filename):
    os.makedirs('./results', exist_ok=True) 
    
    plt.pie(percentages, labels=labels)

    plt.savefig(f'results/{filename}.png')

def lossCurveGraph(loss_curves, labels, filename):
    os.makedirs('./results', exist_ok=True)
    
    for i, loss_curve in enumerate(loss_curves):
        ax = sns.lineplot(loss_curve, label=labels[i])

        ax.set(xlabel='iterations', ylabel='loss')
    
    plt.savefig(f'results/{filename}.png')

def createAccuracyHeatmap(data, xtick, ytick, xlabel, ylabel, colorbarlabel, title, filename):
    os.makedirs('./results', exist_ok=True)
    
    plt.figure(figsize=(10,8))
    ax = sns.heatmap(data=data, yticklabels=ytick, xticklabels=xtick, cbar_kws={'label': colorbarlabel}, cmap='Blues', annot=True, fmt='0.4f')
    ax.set(xlabel=xlabel, ylabel=ylabel, title=title)

    plt.savefig(f'results/{filename}.png')

def createDecisionTree(data, feature_names, title, filename):
    os.makedirs('./results', exist_ok=True)

    plt.figure(figsize=(20, 10))
    plot_tree(data, filled=True, feature_names=feature_names, class_names=True, rounded=True)
    plt.title(title)

    plt.savefig(f'results/{filename}.png')

def create3DPCAPlot(X, y, feature_names, title, filename):
    # Standardizing the features
    X_std = StandardScaler().fit_transform(X)

    # Performing PCA for 3 components
    pca = PCA(n_components=3)
    principalComponents = pca.fit_transform(X_std)
    
    # Print out the feature contributions to each principal component
    # for i, pc in enumerate(['PC1', 'PC2', 'PC3']):
    #     print(f"Feature contributions for {pc}:")
    #     loadings = pca.components_[i]
    #     sorted_indices = np.argsort(abs(loadings))[::-1]
    #     for idx in sorted_indices:
    #         print(f"{feature_names[idx]}: {loadings[idx]:.4f}")

    labels_map = {0: 'Malicious', 1: 'Benign'}
    colors_map = {'Malicious': 'r', 'Benign': 'g'}

    # Creating a DataFrame for the PCA results
    pca_df = pd.DataFrame(data=principalComponents, columns=['PC1', 'PC2', 'PC3'])
    pca_df = pd.concat([pca_df, y.reset_index(drop=True)], axis=1)

    # Plotting in 3D
    fig = plt.figure(figsize=(10,8))
    ax = fig.add_subplot(111, projection='3d')

    for label_num, label_str in labels_map.items():
        indicesToKeep = pca_df[y.name] == label_num
        ax.scatter(pca_df.loc[indicesToKeep, 'PC1'],
                   pca_df.loc[indicesToKeep, 'PC2'],
                   pca_df.loc[indicesToKeep, 'PC3'], 
                   c=colors_map[label_str], s=50, label=label_str)

    # Calculating the boundaries of the main cluster by excluding outliers
    percentile = 95  # Adjust the percentile as necessary
    max_val_pc1 = np.percentile(pca_df['PC1'], percentile)
    max_val_pc2 = np.percentile(pca_df['PC2'], percentile)
    max_val_pc3 = np.percentile(pca_df['PC3'], percentile)

    ax.set_xlim(pca_df['PC1'].min(), max_val_pc1)
    ax.set_ylim(pca_df['PC2'].min(), max_val_pc2)
    ax.set_zlim(pca_df['PC3'].min(), max_val_pc3)

    ax.set_xlabel('Principal Component 1 (Variance: {:.2f}%)'.format(pca.explained_variance_ratio_[0]*100))
    ax.set_ylabel('Principal Component 2 (Variance: {:.2f}%)'.format(pca.explained_variance_ratio_[1]*100))
    ax.set_zlabel('Principal Component 3 (Variance: {:.2f}%)'.format(pca.explained_variance_ratio_[2]*100))
    ax.set_title(title)
    ax.legend()

    os.makedirs('./results', exist_ok=True)
    plt.savefig(f'results/{filename}_3d.png')
