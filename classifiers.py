import argparse
import numpy as np
import pandas as pd

import os
import matplotlib.pyplot as plt

# from sklearn.neural_network import MLPClassifier
# from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
# from sklearn.tree import DecisionTreeClassifier
from sklearn.tree import plot_tree
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
# from sklearn.naive_bayes import GaussianNB

# from sklearn.ensemble import BaggingClassifier

from sklearn.metrics import accuracy_score, confusion_matrix
from sklearn.model_selection import train_test_split

import visualizations as vis

parser = argparse.ArgumentParser("All the classifiers!")
parser.add_argument("--split_test_size", default=0.33, type=float, help="Fraction of data to be used as testing data")
parser.add_argument("--split_seed", default=42, type=int, help="Seed used to determine the train/test split of the data")
args = parser.parse_args()

csv = pd.read_csv('data/balancedData.csv')

y = csv['label']
csv = csv.drop('label', axis=1)
X = csv

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=args.split_test_size, random_state=args.split_seed)

def runModel(max_depth, n_estimators):
    clf = RandomForestClassifier(max_depth=max_depth, n_estimators=n_estimators, max_features=1)
    clf.fit(X_train, y_train)
    y_predict = clf.predict(X_train)
    train_acc = accuracy_score(y_train, y_predict)
    training_cm = confusion_matrix(y_train, y_predict)

    y_predict = clf.predict(X_test)
    test_acc = accuracy_score(y_test, y_predict)

    vis.createDecisionTree(clf.estimator_[0], X_train.columns, 'Decision Tree', 'descision_tree')

    return train_acc, test_acc, training_cm


def testHyperParams(depths, estimators):
    a_training = []
    a_testing = []
    for i in depths:
        d_training = []
        d_testing = []
        for j in estimators:
            train_acc, test_acc, training_cm = runModel(i, j)
            d_training.append(train_acc)
            d_testing.append(test_acc)

        a_training.append(d_training)
        a_testing.append(d_testing)

    accuracies_training = pd.DataFrame(data=a_training, columns=depths, index=estimators)
    accuracies_testing = pd.DataFrame(data=a_testing, columns=depths, index=estimators)

    print(accuracies_training)
    print(accuracies_testing)

    vis.createAccuracyHeatmap(a_training, estimators, depths, 'n_estimators', 'max_depth', 'accuracy score', 'trianing max_depth vs. n_estimators', 'training')
    vis.createAccuracyHeatmap(a_testing, estimators, depths, 'n_estimators', 'max_depth', 'accuracy score', 'testing max_depth vs. n_estimators', 'testing')
    vis.createCorrMatrix(training_cm, y.unique(), 'Random Forest Training Confusion Matrix', f'training_confusion_matrix')


# depths = [200, 100, 50, 10, 5]
# estimators = [5, 10, 50, 100, 200]
# testHyperParams(depths, estimators)

runModel(10, 100)
# VISUALS
# vis.createAccuracyHeatmap(a_training, estimators, depths, 'n_estimators', 'max_depth', 'accuracy score', 'trianing max_depth vs. n_estimators', 'training')
# vis.createAccuracyHeatmap(a_testing, estimators, depths, 'n_estimators', 'max_depth', 'accuracy score', 'testing max_depth vs. n_estimators', 'testing')
# vis.createDecisionTree(clf.estimator_[0], X_train.columns, 'Decision Tree', 'descision_tree')
# vis.createCorrMatrix(training_cm, y.unique(), 'Random Forest Training Confusion Matrix', f'training_confusion_matrix')

# os.makedirs('./results', exist_ok=True)
# plt.figure(figsize=(20, 10))
# plot_tree(clf.estimator_[0], filled=True, feature_names=X_train.columns, class_names=True, rounded=True)
# plt.title('Decision Tree')

# plt.savefig(f'results/descision_tree.png')